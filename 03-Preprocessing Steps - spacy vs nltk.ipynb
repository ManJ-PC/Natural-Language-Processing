{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c9a5f24",
   "metadata": {},
   "source": [
    "#### Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58ebb854",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7592607",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520e8d83",
   "metadata": {},
   "source": [
    "Tokenization is a common task in Natural Language Processing (NLP).Tokens are the building blocks of Natural Language. Tokenization is a way of separating a piece of text into smaller units called token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47ff00b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In|computing|,|plain|text|is|a|loose|term|for|data|that|represent|          |only|characters|of|readable|material|but|not|its|graphical|representation|nor|other|objects|.|"
     ]
    }
   ],
   "source": [
    "doc = nlp('In computing, plain text is a loose term for data that represent \\\n",
    "          only characters of readable material but not its graphical representation nor other objects.')\n",
    "\n",
    "for token in doc:\n",
    "    print(token, end='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d0a73859",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In computing, plain text is a loose term for data that represent           only characters of readable material but not its graphical representation nor other objects.']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token.text for token in doc.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74b794d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Importing NLTK\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "03ee2733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In computing, plain text is a loose term for data that represent           only characters of readable material but not its graphical representation nor other objects.']\n",
      "No of tokens: 1\n"
     ]
    }
   ],
   "source": [
    "sample = 'In computing, plain text is a loose term for data that represent \\\n",
    "          only characters of readable material but not its graphical representation nor other objects.'\n",
    "tokens = nltk.sent_tokenize(sample)\n",
    "print(tokens)\n",
    "print('No of tokens:', len(tokens) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6fc72531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', 'computing', ',', 'plain', 'text', 'is', 'a', 'loose', 'term', 'for', 'data', 'that', 'represent', 'only', 'characters', 'of', 'readable', 'material', 'but', 'not', 'its', 'graphical', 'representation', 'nor', 'other', 'objects', '.']\n",
      "No of tokens: 27\n"
     ]
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(sample)\n",
    "print(tokens)\n",
    "print('No of tokens:', len(tokens) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3d92abdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', 'computing', ',', 'plain', 'text', 'is', 'a', 'loose', 'term', 'for', 'data', 'that', 'represent', 'only', 'characters', 'of', 'readable', 'material', 'but', 'not', 'its', 'graphical', 'representation', 'nor', 'other', 'objects', '.']\n",
      "No of tokens: 27\n"
     ]
    }
   ],
   "source": [
    "tok = nltk.toktok.ToktokTokenizer()\n",
    "tokens = tok.tokenize(sample)\n",
    "print(tokens)\n",
    "print('No of tokens:', len(tokens) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32aaa2bd",
   "metadata": {},
   "source": [
    "#### Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cafd925",
   "metadata": {},
   "source": [
    "Stemming is a somewhat crude method for cataloging related words; it essentially chops off letters from the end until the stem is reached. This works fairly well in most cases, but unfortunately English has many exceptions where a more sophisticated process is required. In fact, spaCy doesn't include a stemmer, opting instead to rely entirely on lemmatization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cee83a",
   "metadata": {},
   "source": [
    "One of the most common and effective stemming tools is Porter's Algorithm developed by Martin Porter in 1980. The algorithm employs five phases of word reduction, each with its own set of mapping rules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed53425",
   "metadata": {},
   "source": [
    "## Porter Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e1f97c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run --> run\n",
      "runner --> runner\n",
      "running --> run\n",
      "ran --> ran\n",
      "runs --> run\n",
      "easily --> easili\n",
      "fairly --> fairli\n"
     ]
    }
   ],
   "source": [
    "# Import the toolkit and the full Porter Stemmer library\n",
    "import nltk\n",
    "\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "p_stemmer = PorterStemmer()\n",
    "\n",
    "words = ['run','runner','running','ran','runs','easily','fairly']\n",
    "\n",
    "for word in words:\n",
    "    print(word+' --> '+p_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1b512d",
   "metadata": {},
   "source": [
    "<font color=green>Note how the stemmer recognizes \"runner\" as a noun, not a verb form or participle. Also, the adverbs \"easily\" and \"fairly\" are stemmed to the unusual root \"easili\" and \"fairli\"</font>\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c34378d",
   "metadata": {},
   "source": [
    "## Snowball Stemmer\n",
    "This is somewhat of a misnomer, as Snowball is the name of a stemming language developed by Martin Porter. The algorithm used here is more accurately called the \"English Stemmer\" or \"Porter2 Stemmer\". It offers a slight improvement over the original Porter stemmer, both in logic and speed. Since **nltk** uses the name SnowballStemmer, we'll use it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0e5ff9fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run --> run\n",
      "runner --> runner\n",
      "running --> run\n",
      "ran --> ran\n",
      "runs --> run\n",
      "easily --> easili\n",
      "fairly --> fair\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# The Snowball Stemmer requires that you pass a language parameter\n",
    "s_stemmer = SnowballStemmer(language='english')\n",
    "words = ['run','runner','running','ran','runs','easily','fairly']\n",
    "\n",
    "for word in words:\n",
    "    print(word+' --> '+s_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9431631b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generous --> generous\n",
      "generation --> generat\n",
      "generously --> generous\n",
      "generate --> generat\n"
     ]
    }
   ],
   "source": [
    "words = ['generous','generation','generously','generate']\n",
    "\n",
    "for word in words:\n",
    "    print(word+' --> '+s_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75602dd5",
   "metadata": {},
   "source": [
    "## Lancaster Stemmer\n",
    "The Lancaster stemming algorithm is another algorithm that you can use. This one is the most aggressive stemming algorithm of the bunch. However, if you use the stemmer in NLTK, you can add your own custom rules to this algorithm very easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f478e635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run --> run\n",
      "runner --> run\n",
      "running --> run\n",
      "ran --> ran\n",
      "runs --> run\n",
      "easily --> easy\n",
      "fairly --> fair\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "ls = LancasterStemmer()\n",
    "\n",
    "words = ['run','runner','running','ran','runs','easily','fairly']\n",
    "\n",
    "for word in words:\n",
    "    print(word+' --> '+ls.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d249bf",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "In contrast to stemming, lemmatization looks beyond word reduction, and considers a language's full vocabulary to apply a *morphological analysis* to words. The lemma of 'was' is 'be' and the lemma of 'mice' is 'mouse'. Further, the lemma of 'meeting' might be 'meet' or 'meeting' depending on its use in a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9c6ce4d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I          -- I         \n",
      "am         -- be        \n",
      "a          -- a         \n",
      "runner     -- runner    \n",
      "running    -- run       \n",
      "in         -- in        \n",
      "a          -- a         \n",
      "race       -- race      \n",
      "because    -- because   \n",
      "I          -- I         \n",
      "love       -- love      \n",
      "to         -- to        \n",
      "run        -- run       \n",
      "since      -- since     \n",
      "I          -- I         \n",
      "ran        -- run       \n",
      "today      -- today     \n"
     ]
    }
   ],
   "source": [
    "##Spacy\n",
    "doc1 = nlp(u\"I am a runner running in a race because I love to run since I ran today\")\n",
    "\n",
    "for token in doc1:\n",
    "    print(f'{token.text:{10}} -- {token.lemma_:{10}}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c629c711",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The brown fox are quick and they are jumping over the sleeping lazy dog !'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##NLTK lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer() #Creating object of word net lemmatizer\n",
    "text = 'The brown foxes are quick and they are jumping over the sleeping lazy dogs!'\n",
    "tokens = nltk.word_tokenize(text)\n",
    "lemmatized_text = ' '.join(wnl.lemmatize(token) for token in tokens)\n",
    "#Nltk's lemmatization method require positional tag to perform well.\n",
    "lemmatized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c84980b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('brown', 'JJ'), ('foxes', 'NNS'), ('are', 'VBP'), ('quick', 'JJ'), ('and', 'CC'), ('they', 'PRP'), ('are', 'VBP'), ('jumping', 'VBG'), ('over', 'IN'), ('the', 'DT'), ('sleeping', 'VBG'), ('lazy', 'JJ'), ('dogs', 'NNS'), ('!', '.')]\n"
     ]
    }
   ],
   "source": [
    "tagged_tokens = nltk.pos_tag(tokens)\n",
    "print(tagged_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "aa516586",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The brown fox be quick and they be jump over the sleep lazy dog !'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "##Positional Tagging the word\n",
    "def pos_tag_wordnet(tagged_tokens):\n",
    "    tag_map = {'j': wordnet.ADJ, 'v': wordnet.VERB, 'n': wordnet.NOUN, 'r': wordnet.ADV}\n",
    "    new_tagged_tokens = [(word, tag_map.get(tag[0].lower(), wordnet.NOUN))\n",
    "                            for word, tag in tagged_tokens]\n",
    "    return new_tagged_tokens\n",
    "\n",
    "#NLTK's lemmatization\n",
    "def wordnet_lemmatize_text(text):\n",
    "    tagged_tokens = nltk.pos_tag(nltk.word_tokenize(text)) #Positonal tagging\n",
    "    wordnet_tokens = pos_tag_wordnet(tagged_tokens)\n",
    "    #Word lemmatizer method don't understand all the tags, so it's converted into basic format \n",
    "    #by passing it to the function\n",
    "    lemmatized_text = ' '.join(wnl.lemmatize(word, tag) for word, tag in wordnet_tokens) #lemmatizing the tokens\n",
    "    return lemmatized_text\n",
    "\n",
    "lemma_words = wordnet_lemmatize_text(text)\n",
    "\n",
    "lemma_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd698fe",
   "metadata": {},
   "source": [
    "## Stop words\n",
    "Words like \"a\" and \"the\" appear so frequently that they don't require tagging as thoroughly as nouns, verbs and modifiers. We call these *stop words*, and they can be filtered from the text to be processed. spaCy holds a built-in list of some 305 English stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92f372ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "327\n"
     ]
    }
   ],
   "source": [
    "print(len(nlp.Defaults.stop_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0e9a75",
   "metadata": {},
   "source": [
    "## To add a stop word\n",
    "There may be times when you wish to add a stop word to the default set. Perhaps you decide that `'btw'` (common shorthand for \"by the way\") should be considered a stop word.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6df2769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default words: 326\n",
      "After adding: 327\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add the word to the set of stop words. Use lowercase!\n",
    "print('Default words:',len(nlp.Defaults.stop_words))\n",
    "nlp.Defaults.stop_words.add('btw')\n",
    "\n",
    "# Set the stop_word tag on the lexeme\n",
    "nlp.vocab['btw'].is_stop = True\n",
    "\n",
    "print('After adding:',len(nlp.Defaults.stop_words))\n",
    "\n",
    "nlp.vocab['btw'].is_stop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcea4c3",
   "metadata": {},
   "source": [
    "<font color=green>When adding stop words, always use lowercase. Lexemes are converted to lowercase before being added to **vocab**.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78353b8a",
   "metadata": {},
   "source": [
    "## To remove a stop word\n",
    "Alternatively, you may decide that `'beyond'` should not be considered a stop word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be0af46e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove the word from the set of stop words\n",
    "nlp.Defaults.stop_words.remove('beyond')\n",
    "\n",
    "# Remove the stop_word tag from the lexeme\n",
    "nlp.vocab['beyond'].is_stop = False\n",
    "\n",
    "print(len(nlp.Defaults.stop_words))\n",
    "\n",
    "nlp.vocab['beyond'].is_stop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6491812",
   "metadata": {},
   "source": [
    "### NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0a882b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default length: 179\n",
      "After removing a word length: 178\n",
      "After adding a word length: 179\n"
     ]
    }
   ],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "print('Default length:', len(stopwords))\n",
    "stopwords.remove('the')\n",
    "print('After removing a word length:', len(stopwords))\n",
    "stopwords.append('brown')\n",
    "print('After adding a word length:', len(stopwords))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
